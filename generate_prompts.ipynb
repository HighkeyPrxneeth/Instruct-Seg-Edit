{
 "cells": [
  {
   "cell_type": "code",
   "id": "3a05d4547ab8a3ca",
   "metadata": {},
   "source": [
    "import asyncio\n",
    "import hashlib\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "import pandas as pd\n",
    "import requests\n",
    "from PIL import Image, ImageFile\n",
    "from requests.adapters import HTTPAdapter\n",
    "from tqdm.notebook import tqdm\n",
    "from urllib3.util.retry import Retry\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Enable loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Configuration - OPTIMIZED FOR RATE LIMITING AND API TIMEOUTS\n",
    "CONFIG = {\n",
    "    \"api_key\": \"api_key\",\n",
    "    \"base_url\": \"http://127.0.0.1:1234/v1\",  # OpenRouter base URL for OpenAI client\n",
    "    \"model\": \"qwen3-vl-1b-merged\",\n",
    "    \"image_size\": (512, 512),\n",
    "    \"invalid_image_size\": (130, 60),\n",
    "    \"images_dir\": Path(\"data/images\"),\n",
    "    \"prompts_dir\": Path(\"data/prompts\"),\n",
    "    \"batch_size\": 10,  # Reduced further to avoid rate limits\n",
    "    \"max_retries\": 5,  # Increased retries\n",
    "    \"retry_delay\": 2.0,  # Increased base delay\n",
    "    \"timeout\": 120,  # Increased to 2 minutes for API calls\n",
    "    \"download_timeout\": 5.0,  # Skip images that take more than 5 seconds to download\n",
    "    \"max_concurrent_requests\": 10,\n",
    "    \"max_workers\": 20,  # Thread pool size for image processing\n",
    "    \"connect_timeout\": 10,  # Connection timeout\n",
    "    \"read_timeout\": 110,  # Read timeout (less than total timeout)\n",
    "    \"download_delay\": 0.5,  # Minimum delay between image downloads (in seconds)\n",
    "    \"batch_delay\": 1.0,  # Delay between batches to prevent rate limiting\n",
    "}\n",
    "\n",
    "# Setup logging - COMPREHENSIVE FIX for duplicate logs\n",
    "def setup_logger():\n",
    "    \"\"\"Set up logger with comprehensive duplicate prevention.\"\"\"\n",
    "    # Use a specific logger name to avoid conflicts\n",
    "    logger_name = \"generate_prompts\"\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    \n",
    "    # Clear ALL handlers from this specific logger AND root logger\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "    \n",
    "    # Also clear root logger handlers to prevent inheritance issues\n",
    "    root_logger = logging.getLogger()\n",
    "    for handler in root_logger.handlers[:]:\n",
    "        root_logger.removeHandler(handler)\n",
    "    \n",
    "    # Set logger level\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Create formatter\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    \n",
    "    # File handler\n",
    "    file_handler = logging.FileHandler(\"generate_prompts.log\")\n",
    "    file_handler.setFormatter(formatter)\n",
    "    file_handler.setLevel(logging.WARNING)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    console_handler.setLevel(logging.WARNING)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    # CRITICAL: Prevent propagation completely\n",
    "    logger.propagate = False\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# Initialize logger\n",
    "logger = setup_logger()\n",
    "\n",
    "# Test that logging is working correctly\n",
    "logger.info(\"Logger setup completed with enhanced rate limiting - this message should appear only once\")\n",
    "\n",
    "# Create directories\n",
    "CONFIG[\"images_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "CONFIG[\"prompts_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Session with connection pooling and retry strategy (for image downloads)\n",
    "# Updated with more conservative settings for rate limiting\n",
    "session = requests.Session()\n",
    "retry_strategy = Retry(\n",
    "    total=CONFIG[\"max_retries\"],\n",
    "    backoff_factor=3,  # More aggressive backoff\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    respect_retry_after_header=True,  # Respect server retry-after headers\n",
    ")\n",
    "adapter = HTTPAdapter(\n",
    "    max_retries=retry_strategy, \n",
    "    pool_maxsize=5,  # Reduced pool size\n",
    "    pool_block=True\n",
    ")\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "# Add rate limiting headers to be more respectful\n",
    "session.headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "29989205c4c48886",
   "metadata": {},
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "**Task:** You are an expert in creating image editing instructions. Your goal is to analyze an image and its accompanying caption to generate a concise instruction that describes a specific, tangible edit to the main subject of the image.\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Focus on the Subject:** The edit instruction must apply *only* to the primary subject of the image.\n",
    "2.  **Describe the Action:** The instruction should describe a clear action or transformation based on the caption.\n",
    "3.  **Be Concise:** Output only the edit instruction itself, with no extra text, labels, or explanations.\n",
    "4.  **Exclude:** Do not mention changes to the background, lighting, contrast, saturation, or overall image style.\n",
    "\n",
    "---\n",
    "**Example:**\n",
    "\n",
    "**Input Caption:**\n",
    "This image displays: a group of nine mountaineers on a snow-covered mountain summit. They are dressed in warm gear and carrying backpacks. There is a mountain in the background and clouds below and behind them. One of the mountaineers is waving. The image is a photograph.\n",
    "\n",
    "**Generated Instruction:**\n",
    "Give the waving mountaineer a soda can in their hand.\n",
    "---\n",
    "\n",
    "Caption:\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6f21520f9f94bb2c",
   "metadata": {},
   "source": [
    "def get_image_hash(url: str) -> str:\n",
    "    \"\"\"Generate a hash for the URL to create unique filenames.\"\"\"\n",
    "    return hashlib.md5(url.encode()).hexdigest()\n",
    "\n",
    "\n",
    "def is_processed(image_hash: str) -> bool:\n",
    "    \"\"\"Check if an image has already been processed.\"\"\"\n",
    "    prompt_file = CONFIG[\"prompts_dir\"] / f\"{image_hash}.txt\"\n",
    "    return prompt_file.exists()\n",
    "\n",
    "\n",
    "def save_prompt(image_hash: str, prompt: str) -> None:\n",
    "    \"\"\"Save the generated prompt to a file.\"\"\"\n",
    "    prompt_file = CONFIG[\"prompts_dir\"] / f\"{image_hash}.txt\"\n",
    "    with open(prompt_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(prompt)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e3f608b82ddee728",
   "metadata": {},
   "source": [
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError\n",
    "import socket\n",
    "\n",
    "def download_and_process_image(\n",
    "    url: str, timeout: float = None\n",
    ") -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"Download and process image from URL with ENFORCED 5-second timeout.\"\"\"\n",
    "    if timeout is None:\n",
    "        timeout = CONFIG.get(\"download_timeout\", 5.0)  # Default 5-second timeout\n",
    "\n",
    "    # Skip all imgur URLs immediately\n",
    "    # if \"imgur\" in url.lower():\n",
    "    #     # logger.info(f\"🚫 Skipping imgur URL: {url}\")\n",
    "    #     return None, None\n",
    "\n",
    "    def _download():\n",
    "        \"\"\"Internal download function.\"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # Configure session with very aggressive timeouts\n",
    "            local_session = requests.Session()\n",
    "            local_session.mount('http://', requests.adapters.HTTPAdapter(max_retries=0))\n",
    "            local_session.mount('https://', requests.adapters.HTTPAdapter(max_retries=0))\n",
    "            \n",
    "            response = local_session.get(\n",
    "                url,\n",
    "                timeout=(2.0, 3.0),  # Very short timeouts (connect, read)\n",
    "                stream=True,\n",
    "                allow_redirects=False,  # No redirects to avoid delays\n",
    "                headers={'User-Agent': 'Mozilla/5.0'}\n",
    "            )\n",
    "            \n",
    "            download_time = time.time() - start_time\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Quick checks\n",
    "            content_type = response.headers.get(\"content-type\", \"\")\n",
    "            if not content_type.startswith(\"image/\"):\n",
    "                return None, download_time, \"not an image\"\n",
    "                \n",
    "            content_length = response.headers.get('content-length')\n",
    "            if content_length and int(content_length) > 10 * 1024 * 1024:  # 10MB limit\n",
    "                return None, download_time, \"too large\"\n",
    "\n",
    "            # Get content quickly\n",
    "            content = response.content\n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            return content, total_time, \"success\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, time.time() - start_time, str(e)\n",
    "        finally:\n",
    "            try:\n",
    "                local_session.close()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    try:\n",
    "        # logger.debug(f\"Downloading: {url} (max {timeout}s)\")\n",
    "        \n",
    "        # Use thread with strict timeout\n",
    "        with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "            future = executor.submit(_download)\n",
    "            \n",
    "            try:\n",
    "                content, download_time, status = future.result(timeout=timeout)\n",
    "                \n",
    "                if content is None:\n",
    "                    # Suppress warning log for failed downloads\n",
    "                    return None, None\n",
    "                \n",
    "                if download_time > 3.0:\n",
    "                    logger.warning(f\"Slow download ({download_time:.1f}s): {url}\")\n",
    "                    pass\n",
    "\n",
    "                # Process image\n",
    "                img = Image.open(BytesIO(content)).convert(\"RGB\")\n",
    "\n",
    "                # Skip placeholders\n",
    "                if img.size == CONFIG[\"invalid_image_size\"]:\n",
    "                    # logger.debug(f\"Skipping placeholder: {url}\")\n",
    "                    return None, None\n",
    "\n",
    "                # Resize and save\n",
    "                img = img.resize(CONFIG[\"image_size\"], Image.Resampling.LANCZOS)\n",
    "                \n",
    "                image_hash = get_image_hash(url)\n",
    "                img_name = f\"{image_hash}.jpg\"\n",
    "                img_path = CONFIG[\"images_dir\"] / img_name\n",
    "                \n",
    "                img.save(img_path, \"JPEG\", quality=95, optimize=True)\n",
    "\n",
    "                # logger.debug(f\"✅ Downloaded: {url} ({download_time:.1f}s)\")\n",
    "                return str(img_path), url\n",
    "                \n",
    "            except FutureTimeoutError:\n",
    "                # logger.warning(f\"⏰ TIMEOUT: {url} (>{timeout}s) - skipping\")\n",
    "                future.cancel()\n",
    "                return None, None\n",
    "                \n",
    "    except Exception as e:\n",
    "        # logger.warning(f\"❌ Error: {url} - {type(e).__name__}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def encode_image_to_base64(image_path: str) -> Optional[str]:\n",
    "    \"\"\"Encode image file to base64 string.\"\"\"\n",
    "    try:\n",
    "        import base64\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            base64_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "        return f\"data:image/jpeg;base64,{base64_string}\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error encoding image to base64: {e}\")\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e08d892ca3b1fcea",
   "metadata": {},
   "source": [
    "async def call_api_async(client: AsyncOpenAI, caption: str, image_path: str, max_retries: int = None) -> Optional[str]:\n",
    "    \"\"\"Make async API call with proper error handling and retries using OpenAI client with base64 encoded images.\"\"\"\n",
    "    if max_retries is None:\n",
    "        max_retries = CONFIG[\"max_retries\"]\n",
    "\n",
    "    # Encode image to base64\n",
    "    base64_image = encode_image_to_base64(image_path)\n",
    "    if not base64_image:\n",
    "        logger.error(f\"Failed to encode image to base64: {image_path}\")\n",
    "        return None\n",
    "\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            logger.debug(f\"Making API call (attempt {attempt + 1}/{max_retries + 1}) for image: {image_path}\")\n",
    "            \n",
    "            response = await client.chat.completions.create(\n",
    "                model=CONFIG[\"model\"],\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": SYSTEM_PROMPT + caption},\n",
    "                            {\"type\": \"image_url\", \"image_url\": {\"url\": base64_image}},\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "                timeout=CONFIG[\"timeout\"]\n",
    "            )\n",
    "            \n",
    "            if response.choices and len(response.choices) > 0:\n",
    "                content = response.choices[0].message.content.strip()\n",
    "                logger.debug(f\"Successfully generated prompt for image: {image_path}\")\n",
    "                return content\n",
    "            else:\n",
    "                logger.error(f\"No choices in API response for image: {image_path}\")\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            error_details = f\"API call error: {type(e).__name__}: {str(e)}\"\n",
    "            \n",
    "            # Handle specific OpenAI errors\n",
    "            if hasattr(e, 'status_code'):\n",
    "                error_details += f\" (status: {e.status_code})\"\n",
    "                \n",
    "                # Rate limiting\n",
    "                if e.status_code == 429:\n",
    "                    if attempt < max_retries:\n",
    "                        wait_time = CONFIG[\"retry_delay\"] * (3**attempt)  # More aggressive backoff\n",
    "                        logger.warning(f\"Rate limited, waiting {wait_time}s before retry\")\n",
    "                        await asyncio.sleep(wait_time)\n",
    "                        continue\n",
    "                        \n",
    "                # Server errors\n",
    "                elif e.status_code >= 500:\n",
    "                    if attempt < max_retries:\n",
    "                        wait_time = CONFIG[\"retry_delay\"] * (2**attempt)\n",
    "                        logger.warning(f\"Server error {e.status_code}, waiting {wait_time}s before retry\")\n",
    "                        await asyncio.sleep(wait_time)\n",
    "                        continue\n",
    "            \n",
    "            # Timeout errors\n",
    "            elif \"timeout\" in str(e).lower():\n",
    "                if attempt < max_retries:\n",
    "                    wait_time = CONFIG[\"retry_delay\"] * (2**attempt)\n",
    "                    logger.warning(f\"API call timed out (attempt {attempt + 1}/{max_retries + 1}) after {CONFIG['timeout']}s. Waiting {wait_time}s before retry...\")\n",
    "                    await asyncio.sleep(wait_time)\n",
    "                    continue\n",
    "                else:\n",
    "                    logger.error(f\"API call timed out after {max_retries + 1} attempts. Consider increasing timeout or reducing concurrent requests.\")\n",
    "                    return None\n",
    "            \n",
    "            # General retry for other errors\n",
    "            if attempt < max_retries:\n",
    "                wait_time = CONFIG[\"retry_delay\"] * (2**attempt)\n",
    "                logger.warning(f\"API call failed (attempt {attempt + 1}/{max_retries + 1}): {error_details}. Waiting {wait_time}s...\")\n",
    "                await asyncio.sleep(wait_time)\n",
    "            else:\n",
    "                stack_trace = traceback.format_exc()\n",
    "                logger.error(f\"API call failed after {max_retries + 1} attempts: {error_details}\\nFull traceback:\\n{stack_trace}\")\n",
    "                return None\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def call_api(caption: str, image_path: str, max_retries: int = None) -> Optional[str]:\n",
    "    \"\"\"Synchronous wrapper for backward compatibility.\"\"\"\n",
    "    async def run_async():\n",
    "        client = AsyncOpenAI(\n",
    "            api_key=CONFIG[\"api_key\"],\n",
    "            base_url=CONFIG[\"base_url\"]\n",
    "        )\n",
    "        return await call_api_async(client, caption, image_path, max_retries)\n",
    "    \n",
    "    return asyncio.run(run_async())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8c16e0502cc3952f",
   "metadata": {},
   "source": [
    "async def process_single_item(client: AsyncOpenAI, data: pd.Series, pbar: tqdm) -> Dict[str, Any]:\n",
    "    \"\"\"Process a single item (image + caption) asynchronously with rate limiting.\"\"\"\n",
    "    result = {\"processed\": 0, \"errors\": 0, \"cached\": 0}\n",
    "    \n",
    "    img_url = data.get(\"url\", \"\")\n",
    "    if not img_url:\n",
    "        result[\"errors\"] = 1\n",
    "        pbar.update(1)\n",
    "        return result\n",
    "\n",
    "    # Create caption\n",
    "    vlm_caption = data.get(\"vlm_caption\", \"\")\n",
    "    original_caption = data.get(\"original_caption\", \"\")\n",
    "    caption = f\"{vlm_caption}\\n{original_caption}\".strip()\n",
    "\n",
    "    if not caption:\n",
    "        logger.warning(f\"Empty caption for URL: {img_url}\")\n",
    "        result[\"errors\"] = 1\n",
    "        pbar.update(1)\n",
    "        return result\n",
    "\n",
    "    # Check if already processed\n",
    "    image_hash = get_image_hash(img_url)\n",
    "    if is_processed(image_hash):\n",
    "        result[\"cached\"] = 1\n",
    "        pbar.update(1)\n",
    "        return result\n",
    "\n",
    "    try:\n",
    "        # Add a small delay before downloading to prevent overwhelming servers\n",
    "        await asyncio.sleep(CONFIG[\"download_delay\"])\n",
    "        \n",
    "        # Download and process image (synchronous - file I/O intensive)\n",
    "        img_path, processed_url = download_and_process_image(img_url)\n",
    "        if img_path is None:\n",
    "            result[\"errors\"] = 1\n",
    "            pbar.update(1)\n",
    "            return result\n",
    "\n",
    "        # Generate prompt via API (asynchronous) - now using image path for base64 encoding\n",
    "        generated_prompt = await call_api_async(client, caption, img_path)\n",
    "        if generated_prompt:\n",
    "            save_prompt(image_hash, generated_prompt)\n",
    "            result[\"processed\"] = 1\n",
    "            logger.info(f\"Generated prompt for {img_url}: {generated_prompt[:100]}...\")\n",
    "        else:\n",
    "            result[\"errors\"] = 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {img_url}: {e}\")\n",
    "        result[\"errors\"] = 1\n",
    "    \n",
    "    pbar.update(1)\n",
    "    return result\n",
    "\n",
    "\n",
    "async def process_batch_async(df_batch: pd.DataFrame, pbar: tqdm) -> Dict[str, Any]:\n",
    "    \"\"\"Process a batch of images concurrently with rate limiting.\"\"\"\n",
    "    stats = {\"processed\": 0, \"skipped\": 0, \"errors\": 0, \"cached\": 0}\n",
    "    \n",
    "    # Create OpenAI client for async operations\n",
    "    client = AsyncOpenAI(\n",
    "        api_key=CONFIG[\"api_key\"],\n",
    "        base_url=CONFIG[\"base_url\"]\n",
    "    )\n",
    "    \n",
    "    # Create semaphore to limit concurrent requests (keep it conservative)\n",
    "    semaphore = asyncio.Semaphore(CONFIG[\"max_concurrent_requests\"])\n",
    "    \n",
    "    async def process_with_semaphore(data):\n",
    "        async with semaphore:\n",
    "            return await process_single_item(client, data, pbar)\n",
    "    \n",
    "    # Process items with rate limiting - process them sequentially if max_concurrent_requests is 1\n",
    "    if CONFIG[\"max_concurrent_requests\"] == 1:\n",
    "        # Sequential processing to be more respectful to servers\n",
    "        results = []\n",
    "        for _, data in df_batch.iterrows():\n",
    "            result = await process_single_item(client, data, pbar)\n",
    "            results.append(result)\n",
    "    else:\n",
    "        # Create tasks for all items in the batch\n",
    "        tasks = [process_with_semaphore(data) for _, data in df_batch.iterrows()]\n",
    "        \n",
    "        # Process all tasks concurrently\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    # Aggregate results\n",
    "    for result in results:\n",
    "        if isinstance(result, Exception):\n",
    "            logger.error(f\"Task failed with exception: {result}\")\n",
    "            stats[\"errors\"] += 1\n",
    "        else:\n",
    "            for key in stats:\n",
    "                stats[key] += result.get(key, 0)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "def process_batch(df_batch: pd.DataFrame, pbar: tqdm) -> Dict[str, Any]:\n",
    "    \"\"\"Synchronous wrapper for batch processing.\"\"\"\n",
    "    try:\n",
    "        # Check if we're already in an event loop\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            # We're in a notebook with an existing event loop\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            return asyncio.run(process_batch_async(df_batch, pbar))\n",
    "        else:\n",
    "            return asyncio.run(process_batch_async(df_batch, pbar))\n",
    "    except RuntimeError:\n",
    "        # No event loop exists, create one\n",
    "        return asyncio.run(process_batch_async(df_batch, pbar))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "82587ade07c1e5d4",
   "metadata": {},
   "source": [
    "def load_data_efficiently() -> pd.DataFrame:\n",
    "    \"\"\"Load parquet files efficiently.\"\"\"\n",
    "    try:\n",
    "        parquet_files = [\n",
    "            \"data/vlm_captions_redcaps_00.parquet\"\n",
    "        ]\n",
    "\n",
    "        dataframes = []\n",
    "        for file in parquet_files:\n",
    "            if os.path.exists(file):\n",
    "                df = pd.read_parquet(file)\n",
    "                dataframes.append(df)\n",
    "                logger.info(f\"Loaded {len(df)} records from {file}\")\n",
    "\n",
    "        if not dataframes:\n",
    "            raise FileNotFoundError(\"No parquet files found\")\n",
    "\n",
    "        df = dataframes[0].sample(n=20000)\n",
    "        logger.info(f\"Total records loaded: {len(df)}\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {e}\")\n",
    "        raise"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e8e7afc00f4bac7",
   "metadata": {},
   "source": [
    "def main():\n",
    "    \"\"\"Main processing function with rate limiting and batch delays.\"\"\"\n",
    "    try:\n",
    "        # Install nest_asyncio for notebook compatibility\n",
    "        try:\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "        except ImportError:\n",
    "            logger.warning(\"nest_asyncio not available. Install it with: pip install nest-asyncio\")\n",
    "        \n",
    "        # Load data\n",
    "        logger.info(\"Loading data...\")\n",
    "        df = load_data_efficiently()\n",
    "\n",
    "        # Process in batches\n",
    "        total_batches = (len(df) + CONFIG[\"batch_size\"] - 1) // CONFIG[\"batch_size\"]\n",
    "        overall_stats = {\"processed\": 0, \"skipped\": 0, \"errors\": 0, \"cached\": 0}\n",
    "\n",
    "        logger.info(f\"Processing {len(df)} records in {total_batches} batches\")\n",
    "        logger.info(f\"Rate limiting settings: {CONFIG['max_concurrent_requests']} concurrent, {CONFIG['download_delay']}s download delay, {CONFIG['batch_delay']}s batch delay\")\n",
    "\n",
    "        # Create a single progress bar for the entire process\n",
    "        with tqdm(\n",
    "            total=len(df), \n",
    "            desc=\"Generating edit instructions\",\n",
    "            unit=\"images\",\n",
    "            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}\"\n",
    "        ) as pbar:\n",
    "            \n",
    "            for batch_idx in range(total_batches):\n",
    "                start_idx = batch_idx * CONFIG[\"batch_size\"]\n",
    "                end_idx = min(start_idx + CONFIG[\"batch_size\"], len(df))\n",
    "                df_batch = df.iloc[start_idx:end_idx]\n",
    "\n",
    "                # logger.info(f\"Processing batch {batch_idx + 1}/{total_batches} ({len(df_batch)} items)\")\n",
    "                \n",
    "                # Process batch with rate limiting\n",
    "                batch_stats = process_batch(df_batch, pbar)\n",
    "\n",
    "                # Update overall statistics\n",
    "                for key in overall_stats:\n",
    "                    overall_stats[key] += batch_stats[key]\n",
    "\n",
    "                # Update progress bar postfix with current stats\n",
    "                pbar.set_postfix({\n",
    "                    \"Processed\": overall_stats[\"processed\"],\n",
    "                    \"Cached\": overall_stats[\"cached\"], \n",
    "                    \"Errors\": overall_stats[\"errors\"],\n",
    "                    \"Success Rate\": f\"{(overall_stats['processed'] / max(1, overall_stats['processed'] + overall_stats['errors']) * 100):.1f}%\"\n",
    "                })\n",
    "\n",
    "                # Add delay between batches to prevent overwhelming servers\n",
    "                # if batch_idx < total_batches - 1:  # Don't delay after the last batch\n",
    "                #     logger.info(f\"Waiting {CONFIG['batch_delay']}s between batches to prevent rate limiting...\")\n",
    "                #     time.sleep(CONFIG[\"batch_delay\"])\n",
    "\n",
    "        # Print final statistics\n",
    "        logger.info(\"Processing completed!\")\n",
    "        logger.info(f\"Final Statistics:\")\n",
    "        logger.info(f\"  - Processed: {overall_stats['processed']}\")\n",
    "        logger.info(f\"  - Cached: {overall_stats['cached']}\")\n",
    "        logger.info(f\"  - Errors: {overall_stats['errors']}\")\n",
    "        logger.info(f\"  - Success Rate: {(overall_stats['processed'] / max(1, overall_stats['processed'] + overall_stats['errors']) * 100):.1f}%\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Processing interrupted by user\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error in main: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        session.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2a41eb1e3d8e036",
   "metadata": {},
   "source": [
    "main()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
